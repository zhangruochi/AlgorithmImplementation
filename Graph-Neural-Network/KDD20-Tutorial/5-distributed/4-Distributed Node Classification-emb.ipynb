{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed GraphSage with Embeddings for Node Classification\n",
    "\n",
    "The tutorial shows distributed training on GraphSage for node classification on a graph without node features. In this case, we put learnable embeddings on nodes. We reuse code from the mini-batch training examples. The model implementation and sampling for single-machine training and distributed training is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "To help us convert this Notebook to a training script easily, let's list all hyperparameters we want to tune. When we convert the notebook into a training script, we can specify the hyperparameters with arguments of the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_config = None\n",
    "conf_path = 'standalone_data/ogbn-products.json'\n",
    "num_epochs = 1\n",
    "num_hidden = 128\n",
    "num_layers = 2\n",
    "batch_size = 1000\n",
    "batch_size_eval = 100000\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "standalone = True\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the arguments for the training script.\n",
    "\n",
    "**Note**: `argparse` doesn't work in the Jupyter Notebook. When running in the Notebook environment, we should skip executing the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='GCN')\n",
    "parser.add_argument('--ip_config', type=str, help='The file for IP configuration')\n",
    "parser.add_argument('--conf_path', type=str, help='The path to the partition config file')\n",
    "parser.add_argument('--num-epochs', type=int, default=1)\n",
    "parser.add_argument('--num-hidden', type=int, default=128)\n",
    "parser.add_argument('--num-layers', type=int, default=2)\n",
    "parser.add_argument('--batch-size', type=int, default=1000)\n",
    "parser.add_argument('--batch-size-eval', type=int, default=100000)\n",
    "parser.add_argument('--standalone', action='store_true')\n",
    "parser.add_argument('--local_rank', type=int, help='the rank for distributed training in Pytorch')\n",
    "parser.add_argument('--num-workers', type=int, default=0, help='The number of worker processes for sampling.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "ip_config = args.ip_config\n",
    "conf_path = args.conf_path\n",
    "num_epochs = args.num_epochs\n",
    "num_hidden = args.num_hidden\n",
    "num_layers = args.num_layers\n",
    "batch_size = args.batch_size\n",
    "batch_size_eval = args.batch_size_eval\n",
    "standalone = args.standalone\n",
    "num_workers = args.num_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define user-defined functions\n",
    "\n",
    "Some functions may run both on DGL server and trainer processes. We need to define these functions before we initialize network communication in DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_emb(shape, dtype):\n",
    "    arr = th.zeros(shape, dtype=dtype)\n",
    "    arr.uniform_(-1, 1)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network communication\n",
    "\n",
    "Before creating any components for distributed training, we need to initialize the network communication for both Pytorch and DGL.\n",
    "\n",
    "Initialize RPC for network communication in DGL. When the process runs in the server mode, `init_rpc` will not return. Instead, it executes DGL servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.distributed.initialize(ip_config, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize distributed training in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not standalone:\n",
    "    th.distributed.init_process_group(backend='gloo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DistGraph\n",
    "\n",
    "When creating a DistGraph object, it will load the input graph or connected to the servers that load the input graph, depending on its execution mode.\n",
    "\n",
    "*Note*: the input graph has to be partitioned by the partition notebook first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.distributed.DistGraph('ogbn-products', part_config=conf_path)\n",
    "print('#nodes:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the nodes in the training, validation and testing set, which the current process is responsible for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nid = dgl.distributed.node_split(g.ndata['train_mask'])\n",
    "valid_nid = dgl.distributed.node_split(g.ndata['val_mask'])\n",
    "test_nid = dgl.distributed.node_split(g.ndata['test_mask'])\n",
    "print('train set:', len(train_nid))\n",
    "print('valid set:', len(valid_nid))\n",
    "print('test set:', len(test_nid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a model to classify nodes, we need to know how many unique labels there are in the dataset. The operation below actually fetch the labels of all nodes in the graph and run `unique` on the labels. This operation can be relatively expensive. If a user knows how many labels there are in the dataset, he/she can just pass the number of unique labels as an argument in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = g.ndata['labels'][0:g.number_of_nodes()]\n",
    "uniq_labels = th.unique(labels)\n",
    "num_labels = len(uniq_labels)\n",
    "print('#labels:', num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "The code of defining the GraphSage model is copied from the mini-batch training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, blocks, x):\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            x = layer(block, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and use Adam as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = SAGE(num_hidden, num_hidden, num_labels, num_layers)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable distributed training in Pytroch, we need to convert the model into a distributed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not standalone:\n",
    "    model = th.nn.parallel.DistributedDataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed sampling\n",
    "\n",
    "The same sampling code for a single-process training also works for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([10, 25])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, train_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, valid_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "We need to create learnable embeddings on all nodes with `DistEmbedding`. To train the learnable embeddings, we need to create an optimizer that can update the distributed embeddings in a cluster of machines. The optimizer only updates the embeddings involved in a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = dgl.distributed.DistEmbedding(g.number_of_nodes(), num_hidden, 'emb', init_emb)\n",
    "sparse_opt = dgl.distributed.SparseAdagrad([emb], lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is also the same as the mini-batch training in a single machine.\n",
    "\n",
    "We recommend users to compute the validation score in a mini-batch fashion with neighbor sampling. This is the most cost-effective way of computing validation scores in the distributed training. Although the score could be a little lower than the actual one, it should be sufficient for us to select the right model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over the dataloader to sample the computation dependency graph as a list of\n",
    "    # blocks.\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(train_dataloader):\n",
    "        # Load the input features as well as output labels\n",
    "        batch_inputs = emb(input_nodes)\n",
    "        batch_labels = g.ndata['labels'][seeds]\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        batch_pred = model(blocks, batch_inputs)\n",
    "        loss = loss_fcn(batch_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # Aggregate gradients in multiple nodes.\n",
    "        if not standalone:\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    th.distributed.all_reduce(param.grad.data,\n",
    "                                              op=th.distributed.ReduceOp.SUM)\n",
    "                    param.grad.data /= dgl.distributed.get_num_client()\n",
    "\n",
    "        optimizer.step()\n",
    "        sparse_opt.step()\n",
    "    print('Epoch {}: training takes {:.3f} seconds, loss={:.3f}'.format(epoch, time.time() - start, np.mean(losses)))\n",
    "    \n",
    "    # validation\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    start = time.time()\n",
    "    with th.no_grad():\n",
    "        for step, (input_nodes, seeds, blocks) in enumerate(valid_dataloader):\n",
    "            inputs = emb(input_nodes)\n",
    "            labels.append(g.ndata['labels'][seeds].numpy())\n",
    "            predictions.append(model(blocks, inputs).argmax(1).numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {}: validation takes {:.3f} seconds, Validation Accuracy {}'.format(epoch, time.time() - start, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "The code below implements the distributed full graph inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = dgl.distributed.node_split(np.ones(g.number_of_nodes()),\n",
    "                                   g.get_partition_book(), force_even=True)\n",
    "y = dgl.distributed.DistTensor((g.number_of_nodes(), num_hidden), th.float32, name='h1')\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([None])\n",
    "test_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, nodes, sampler,\n",
    "    batch_size=10000,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "x = emb\n",
    "layers = model.layers if standalone else model.module.layers\n",
    "for l, layer in enumerate(layers):\n",
    "    if l == len(layers) - 1:\n",
    "        y = dgl.distributed.DistTensor((g.number_of_nodes(), num_labels), th.float32, name='h2')\n",
    "    for input_nodes, seeds, blocks in test_dataloader:\n",
    "        block = blocks[0]\n",
    "        with th.no_grad():\n",
    "            if isinstance(x, dgl.distributed.DistTensor):\n",
    "                h = x[input_nodes]\n",
    "            else:\n",
    "                h = x(input_nodes)\n",
    "            h = layer(block, h)\n",
    "            if l != len(layers) - 1:\n",
    "                h = F.relu(h)\n",
    "            y[seeds] = h\n",
    "    x = y\n",
    "    g.barrier()\n",
    "\n",
    "predictions = y[test_nid].argmax(1).numpy()\n",
    "labels = g.ndata['labels'][test_nid]\n",
    "accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "print('Test takes {:.3f} seconds, acc={:.3f}'.format(time.time() - start, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
