{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed GraphSage for Node Classification\n",
    "\n",
    "The tutorial shows distributed training on GraphSage for node classification. We reuse code from the mini-batch training examples. The model implementation and sampling for single-machine training and distributed training is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch as th\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us convert this Notebook to a training script easily, let's list all hyperparameters we want to tune. When we convert the notebook into a training script, we can specify the hyperparameters with arguments of the training script.\n",
    "\n",
    "`standalone` controls whether to call Pytorch distributed training components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_config = None\n",
    "conf_path = 'standalone_data/ogbn-products.json'\n",
    "num_epochs = 10\n",
    "num_hidden = 128\n",
    "num_layers = 2\n",
    "batch_size = 1000\n",
    "batch_size_eval = 100000\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "standalone = True\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the arguments for the training script.\n",
    "\n",
    "**Note**: `argparse` doesn't work in the Jupyter Notebook. When running in the Notebook environment, we should skip executing the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='GCN')\n",
    "parser.add_argument('--ip_config', type=str, help='The file for IP configuration')\n",
    "parser.add_argument('--conf_path', type=str, help='The path to the partition config file')\n",
    "parser.add_argument('--num-epochs', type=int, default=10)\n",
    "parser.add_argument('--num-hidden', type=int, default=128)\n",
    "parser.add_argument('--num-layers', type=int, default=2)\n",
    "parser.add_argument('--batch-size', type=int, default=1000)\n",
    "parser.add_argument('--batch-size-eval', type=int, default=100000)\n",
    "parser.add_argument('--standalone', action='store_true')\n",
    "parser.add_argument('--local_rank', type=int, help='the rank for distributed training in Pytorch')\n",
    "parser.add_argument('--num-workers', type=int, default=0, help='The number of worker processes for sampling.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "ip_config = args.ip_config\n",
    "conf_path = args.conf_path\n",
    "num_epochs = args.num_epochs\n",
    "num_hidden = args.num_hidden\n",
    "num_layers = args.num_layers\n",
    "batch_size = args.batch_size\n",
    "batch_size_eval = args.batch_size_eval\n",
    "standalone = args.standalone\n",
    "num_workers = args.num_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network communication\n",
    "\n",
    "Before creating any components for distributed training, we need to initialize the network communication for both Pytorch and DGL.\n",
    "\n",
    "Initialize RPC for network communication in DGL. When the process runs in the server mode, `initialize` will not return. Instead, it executes DGL servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl.distributed.initialize(ip_config, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize distributed training in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not standalone:\n",
    "    th.distributed.init_process_group(backend='gloo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DistGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a DistGraph object, it will load the input graph or connected to the servers that load the input graph, depending on its execution mode.\n",
    "\n",
    "*Note*: the input graph has to be partitioned by the partition notebook first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.distributed.DistGraph('ogbn-products', part_config=conf_path)\n",
    "print('#nodes:', g.number_of_nodes())\n",
    "print('#edges:', g.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the nodes in the training, validation and testing set, which the current process is responsible for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nid = dgl.distributed.node_split(g.ndata['train_mask'])\n",
    "valid_nid = dgl.distributed.node_split(g.ndata['val_mask'])\n",
    "test_nid = dgl.distributed.node_split(g.ndata['test_mask'])\n",
    "print('train set:', len(train_nid))\n",
    "print('valid set:', len(valid_nid))\n",
    "print('test set:', len(test_nid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a model to classify nodes, we need to know how many unique labels there are in the dataset. The operation below actually fetch the labels of all nodes in the graph and run `unique` on the labels. This operation can be relatively expensive. If a user knows how many labels there are in the dataset, he/she can just pass the number of unique labels as an argument in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = g.ndata['labels'][0:g.number_of_nodes()]\n",
    "uniq_labels = th.unique(labels)\n",
    "num_labels = len(uniq_labels)\n",
    "print('#labels:', num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "The code of defining the GraphSage model is copied from the mini-batch training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, blocks, x):\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            x = layer(block, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and use Adam as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = SAGE(g.ndata['features'].shape[1], num_hidden, num_labels, num_layers)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable distributed training in Pytroch, we need to convert the model into a distributed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not standalone:\n",
    "    model = th.nn.parallel.DistributedDataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same sampling code for a single-process training also works for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([25,10])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, train_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, valid_nid, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is also the same as the mini-batch training in a single machine.\n",
    "\n",
    "We recommend users to compute the validation score in a mini-batch fashion with neighbor sampling. This is the most cost-effective way of computing validation scores in the distributed training. Although the score could be a little lower than the actual one, it should be sufficient for us to select the right model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sklearn.metrics\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over the dataloader to sample the computation dependency graph as a list of blocks.\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for step, (input_nodes, seeds, blocks) in enumerate(train_dataloader):\n",
    "        # Load the input features as well as output labels\n",
    "        batch_inputs = g.ndata['features'][input_nodes]\n",
    "        batch_labels = g.ndata['labels'][seeds]\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        batch_pred = model(blocks, batch_inputs)\n",
    "        loss = loss_fcn(batch_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        # Aggregate gradients in multiple nodes.\n",
    "        if not standalone:\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    th.distributed.all_reduce(param.grad.data,\n",
    "                                              op=th.distributed.ReduceOp.SUM)\n",
    "                    param.grad.data /= dgl.distributed.get_num_client()\n",
    "\n",
    "        optimizer.step()\n",
    "    print('Epoch {}: training takes {:.3f} seconds, loss={:.3f}'.format(epoch, time.time() - start, np.mean(losses)))\n",
    "\n",
    "    # validation\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    start = time.time()\n",
    "    with th.no_grad():\n",
    "        for step, (input_nodes, seeds, blocks) in enumerate(valid_dataloader):\n",
    "            inputs = g.ndata['features'][input_nodes]\n",
    "            labels.append(g.ndata['labels'][seeds].numpy())\n",
    "            predictions.append(model(blocks, inputs).argmax(1).numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {}: validation takes {:.3f} seconds, Validation Accuracy {}'.format(epoch, time.time() - start, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "For offline inference, there are two ways:\n",
    "* We can compute the classification accuracy of nodes in the test set with a mini-batch fashion. In this case, we should still use neighbor sampling to reduce the computation overhead. This is cost effective if the test set is small. However, in practice, the nodes where we need to compute the scores are usually much more than the labeled nodes in the training set. In this case, the mini-batch inference is not recommended.\n",
    "* We can perform the inference on the full graph. In this case, we compute the node embeddings of all nodes in the graph. To perform full graph inference efficiently, we compute the intermediate node embeddings on all nodes layer by layer. In the end, we will compute the final embeddings of all nodes in the graph. After having the final node embeddings, we compute the accuracy on nodes in the test set.\n",
    "\n",
    "![Inference](https://raw.githubusercontent.com/dglai/KDD20-Hands-on-Tutorial/master/asset/inference.png)\n",
    "\n",
    "The code below shows how the full graph inference is implemented in a distributed fashion.\n",
    "\n",
    "First, we split nodes that need to compute embeddings. Since this is full graph inference, all nodes need to compute embeddings, so we generate a boolean array of the size equal to the number of nodes in the graph and all elements are True. `node_split` returns the nodes that the local process is responsible for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = dgl.distributed.node_split(np.ones(g.number_of_nodes(), dtype=bool), g.get_partition_book())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we compute the node embeddings in a layer-by-layer fashion, we need a sampler that samples one-hop neighborhood. We can use a relatively large batch size to increase computation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sampler generates a mini-batch for 1-layer GraphSage. Thus, we can use very large batch size.\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([None])\n",
    "test_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    g, nodes, sampler,\n",
    "    batch_size=10000,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed Pytorch model has slightly different interface. We can access the original model object from its `module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = model.layers if standalone else model.module.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to compute node embeddings one layer at time. It first computes the intermediate node embeddings of all nodes before moving to the next layer. The intermediate embeddings are stored in `DistTensor`.\n",
    "\n",
    "In distributed inference, we have to put a barrier between every layer because different processes may perform computation at a different rate. After computing the node embeddings of one layer, all processes need to synchronize to ensure that the embeddings of all nodes are ready before moving to the next layer. Otherwise, some process that run faster may end up reading embeddings that haven't been computed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x = g.ndata['features']\n",
    "# We create a distributed tensor to store the intermediate node embeddings.\n",
    "y = dgl.distributed.DistTensor((g.number_of_nodes(), num_hidden), th.float32)\n",
    "for l, layer in enumerate(layers):\n",
    "    if l == len(layers) - 1:\n",
    "        # We create another tensor to store the final node embeddings.\n",
    "        y = dgl.distributed.DistTensor((g.number_of_nodes(), num_labels), th.float32)\n",
    "    for input_nodes, seeds, blocks in test_dataloader:\n",
    "        block = blocks[0]\n",
    "        h = x[input_nodes]\n",
    "        with th.no_grad():\n",
    "            h = layer(block, h)\n",
    "            if l != len(layers) - 1:\n",
    "                h = F.relu(h)\n",
    "            y[seeds] = h\n",
    "    x = y\n",
    "    # In the distributed inference, different processes may run at a different rate.\n",
    "    # After computing the node embeddings of one layer, we need to synchronize to ensure\n",
    "    # that we have all the node embeddings ready before moving to the next layer. Otherwise,\n",
    "    # we may end up reading embeddings that haven't been computed yet.\n",
    "    g.barrier()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the node embeddings of all nodes, we can predict the labels of the nodes in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of nodes in the test set.\n",
    "predictions = y[test_nid].argmax(1).numpy()\n",
    "labels = g.ndata['labels'][test_nid]\n",
    "accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "print('Test takes {:.3f} seconds, acc={:.3f}'.format(end - start, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
