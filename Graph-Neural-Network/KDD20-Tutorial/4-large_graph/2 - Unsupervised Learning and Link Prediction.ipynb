{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Link Prediction on Large Graphs\n",
    "\n",
    "In this tutorial you will learn how to train a multi-layer GraphSAGE in an unsupervised learning setting via link prediction on Amazon Copurchase Network provided by OGB.  The dataset contains 2.4 million nodes and 61 million edges, hence not able to fit in a single GPU.\n",
    "\n",
    "The contents in this tutorial include how to \n",
    "\n",
    "* Train a GNN model with a single machine with a single GPU on a graph of any size.\n",
    "* Train a GNN model for link prediction.\n",
    "* Train a GNN model for unsupervised learning.\n",
    "\n",
    "This tutorial is based on the data downloaded in the previous tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction Overview\n",
    "\n",
    "The objective of link prediction is to predict whether an edge exists between two given nodes.  We often formulate the problem as predicting a score $s_{uv} = \\phi(\\boldsymbol{h}^{(l)}_u, \\boldsymbol{h}^{(l)}_v)$ corresponding to the likelihood of an edge existing between two nodes.  We train the model via *negative sampling*, i.e. comparing the score of a real edge against that of a \"non-existent\" edge.\n",
    "\n",
    "A common loss function is negative log-likelihood.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\log \\sigma\\left(s_{uv}\\right) - Q \\mathbb{E}_{v^- \\in P^-(v)}\\left[ \\sigma\\left(-s_{uv^-}\\right) \\right]\n",
    "$$\n",
    "\n",
    "You can also use other loss functions such as BPR or margin loss.\n",
    "\n",
    "Note that the formulation is very similar to that in implicit matrix factorization or word embedding learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Unsupervised Learning with GNNs\n",
    "\n",
    "Link prediction itself is already useful in various tasks such as recommendation where you will predict whether a node will interact with another node.  It is also useful in an unsupervised learning setting where you just want to learn a latent representation of all the nodes.\n",
    "\n",
    "The model will be trained in an unsupervised manner by predicting whether two nodes are connected with an edge, and the learned representations could be used later for nearest neighbor search or future training of a classifier.  The objective function can also be combined together with supervised cross-entropy loss for nodee classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "We directly load the dataset preprocessed by the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "utils.prepare_mp(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Loader with Neighbor Sampling\n",
    "\n",
    "Different from node classification, we need to iterate over edges, then compute the output representation of incident nodes using neighbor sampling and GNN.\n",
    "\n",
    "DGL also provides `EdgeDataLoader` allowing you to iterate over edges for edge classification or link prediction tasks.  To perform link prediction, you need to provide a negative sampler.\n",
    "\n",
    "For homogeneous graphs, the negative sampler can be any callable that has the following signature:\n",
    "\n",
    "```python\n",
    "def negative_sampler(g: DGLGraph, eids: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    pass\n",
    "```\n",
    "\n",
    "The first argument is the original graph and the second argument is the minibatch of edge IDs.  The function returns a pair of $u$-$v^-$ node ID tensors as negative examples.\n",
    "\n",
    "The following code implements a negative sampler that find non-existent edges by sampling `k` $v^-$ for each $u$ according to a distribution $P^-(v) \\propto d(v)^{0.75}$, where $d(v)$ is the degree of $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k):\n",
    "        self.k = k\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        dst = self.weights.multinomial(len(src), replacement=True)\n",
    "        return src, dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the negative sampler, one can then define the edge data loader with neighbor sampling.  Here we will be taking 5 negative examples per positive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "k = 5\n",
    "train_dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "    graph, torch.arange(graph.number_of_edges()), sampler,\n",
    "    negative_sampler=NegativeSampler(graph, k),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peek one minibatch from `train_dataloader` and see what it will give you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example minibatch consists of four elements.\n",
    "\n",
    "* The input node list necessary for computing the representation of output nodes.\n",
    "* The subgraph induced by the nodes being sampled in the minibatch (including those in the negative examples) as well as the edges sampled in the minibatch.\n",
    "* The subgraph induced by the nodes being sampled in the minibatch (including those in the negative examples) as well as the non-existent edges sampled by the negative sampler.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes, pos_graph, neg_graph, blocks = example_minibatch\n",
    "print('Number of input nodes:', len(input_nodes))\n",
    "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
    "print('Negative graph # noeds:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model for Node Representation\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        \n",
    "    def forward(self, blocks, x):\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            x = layer(block, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Node Representation from GNN\n",
    "\n",
    "In the previous tutorial we talked about offline inference of a graph neural network without neighbor sampling.  This can be directly copy-pasted for computing the node representation output from a GNN under an unsupervised learning setting as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = torch.zeros(graph.number_of_nodes(), model.n_hidden)\n",
    "\n",
    "            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):\n",
    "                block = blocks[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(block, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Score Predictor for Edges\n",
    "\n",
    "After getting the node representation necessary for the minibatch, we would like to predict the score of the edges and non-existent edges in the sampled minibatch.  This can be easily accomplished with `apply_edges` method.  Here, we will simply compute the score by dot product of the representations of both incident nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, subgraph, x):\n",
    "        with subgraph.local_scope():\n",
    "            subgraph.ndata['x'] = x\n",
    "            subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'))\n",
    "            return subgraph.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance of the Learned Embedding\n",
    "\n",
    "In this tutorial we will be evaluating the performance of the output embedding by training a linear classifier with the output embedding as input on the training set, and compute the accuracy on the validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "def evaluate(emb, label, train_nids, valid_nids, test_nids):\n",
    "    classifier = sklearn.linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', verbose=1, max_iter=1000)\n",
    "    classifier.fit(emb[train_nids], label[train_nids])\n",
    "    valid_pred = classifier.predict(emb[valid_nids])\n",
    "    test_pred = classifier.predict(emb[test_nids])\n",
    "    valid_acc = sklearn.metrics.accuracy_score(label[valid_nids], valid_pred)\n",
    "    test_acc = sklearn.metrics.accuracy_score(label[test_nids], test_pred)\n",
    "    return valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(node_features.shape[1], 128, 3).cuda()\n",
    "predictor = ScorePredictor().cuda()\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the training loop for unsupervised learning and evaluation, and also saves the model that performs the best on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, pos_graph, neg_graph, blocks) in enumerate(tq):\n",
    "            blocks = [b.to(torch.device('cuda')) for b in blocks]\n",
    "            pos_graph = pos_graph.to(torch.device('cuda'))\n",
    "            neg_graph = neg_graph.to(torch.device('cuda'))\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            outputs = model(blocks, inputs)\n",
    "            pos_score = predictor(pos_graph, outputs)\n",
    "            neg_score = predictor(neg_graph, outputs)\n",
    "            \n",
    "            score = torch.cat([pos_score, neg_score])\n",
    "            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    emb = inference(model, graph, node_features, 16384)\n",
    "    valid_acc, test_acc = evaluate(emb.numpy(), node_labels.numpy())\n",
    "    print('Epoch {} Validation Accuracy {} Test Accuracy {}'.format(epoch, valid_acc, test_acc))\n",
    "    if best_accuracy < valid_acc:\n",
    "        best_accuracy = valid_acc\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for unsupervised learning via link prediction on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "The next tutorial will be about scaling the training procedure out to multiple GPUs on a single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
