{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN with Multiple GPUs\n",
    "\n",
    "In this tutorial you will learn how to train a multi-layer GraphSAGE for node classification on Amazon Copurchase Network provided by OGB with multiple GPUs.  The dataset contains 2.4 million nodes and 61 million edges, hence not able to fit in a single GPU.\n",
    "\n",
    "The contents in this tutorial include how to\n",
    "\n",
    "* Train a GNN model with a single machine with multiple GPUs on a graph of any size with `torch.nn.parallel.DistributedDataParallel`.\n",
    "\n",
    "PyTorch `DistributedDataParallel` (or DDP in short) is a common solution for multi-GPU training.  It is easy to combine DGL with PyTorch DDP, as you do the same thing as that in any ordinary PyTorch applications:\n",
    "\n",
    "* Divide the data to each GPU.\n",
    "* Distribute the model parameters using PyTorch DDP.\n",
    "* Customize your neighborhood sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The following code is copied from the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    import pickle\n",
    "\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "    utils.prepare_mp(graph)\n",
    "    \n",
    "    num_features = node_features.shape[1]\n",
    "    num_classes = (node_labels.max() + 1).item()\n",
    "    \n",
    "    return graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Neighborhood Sampling\n",
    "\n",
    "Previously we have seen how to use `NodeDataLoader` together with `MultiLayerNeighborSampler`.  In fact, you can replace `MultiLayerNeighborSampler` with your own sampling strategy.\n",
    "\n",
    "The customization is simple.  For each GNN layer, you only need to specify the edges involved in the message passing as a graph.  For example, here is how `MultiLayerNeighborSampler` is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, fanouts):\n",
    "        super().__init__(len(fanouts), return_eids=False)\n",
    "        self.fanouts = fanouts\n",
    "        \n",
    "    def sample_frontier(self, block_id, g, seed_nodes):\n",
    "        fanout = self.fanouts[block_id]\n",
    "        return dgl.sampling.sample_neighbors(g, seed_nodes, fanout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Loader for Distributed Data Parallel (DDP)\n",
    "\n",
    "In PyTorch DDP each worker process is assigned an integer *rank*.  The rank would indicate which partition of the dataset the worker process will handle.  So the only difference between single GPU and multiple GPU training in terms of data loader is that the data loader will only iterate over a partition of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids):\n",
    "    partition_size = len(nids) // world_size\n",
    "    partition_offset = partition_size * rank\n",
    "    nids = nids[partition_offset:partition_offset+partition_size]\n",
    "    \n",
    "    sampler = MultiLayerNeighborSampler([4, 4, 4])\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model implementation will be exactly the same as what you have seen in the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, blocks, x):\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            x = layer(block, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing the Model to GPUs\n",
    "\n",
    "PyTorch DDP manages the distribution of models and synchronization of the gradients for you.  In DGL, you can benefit from PyTorch DDP as well by simply wrapping the model with `torch.nn.parallel.DistributedDataParallel`.\n",
    "\n",
    "The recommended way to distribute training is to have one training process per GPU, so during model instantiation we also specify the process rank, which is equal to the GPU ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers):\n",
    "    model = SAGE(in_feats, n_hidden, n_classes, n_layers).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop for one Process\n",
    "\n",
    "The training loop looks the same as other PyTorch DDP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.fix_openmp\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids)\n",
    "    \n",
    "    model = init_model(rank, num_features, 128, num_classes, 3)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "            blocks = [b.to(rank) for b in blocks]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(blocks, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, blocks in valid_dataloader:\n",
    "                    blocks = [b.to(rank) for b in blocks]\n",
    "                    inputs = node_features[input_nodes].cuda()\n",
    "                    labels.append(node_labels[output_nodes].numpy())\n",
    "                    predictions.append(model.module(blocks, inputs).argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = load_data()\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for node classification on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with *any number of* GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
