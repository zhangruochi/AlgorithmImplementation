{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer-XL model was proposed in Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. It’s a causal (uni-directional) transformer with relative positioning (sinusoïdal) embeddings which can reuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax inputs and outputs (tied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5365f8e3378d45dd876f55e100a8bbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9143613.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b57d08c63748f1b2e5503b61a53643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=659.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d992cd3709ce44cdb8fb6d3ad52cff42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1140884800.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jay chou is a\"\n",
    "tokens_tensor = torch.tensor(tokenizer.encode(text)).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prediction_scores (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size)):\n",
    "> Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "\n",
    "- mems (List[torch.FloatTensor] of length config.n_layers):\n",
    "> Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see past input) to speed up sequential decoding. The token ids which have their past given to this model should not be passed as input ids as they have already been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sequence: Jay chou is a\n",
      "Predicted output: type of a type of a type of a type of a type of a type of a type of a type of a type of or a type of a or of a or of a or of a or of of a or of of or of the\n"
     ]
    }
   ],
   "source": [
    "mems = None  # recurrence mechanism\n",
    "\n",
    "predicted_tokens = list()\n",
    "\n",
    "for i in range(50):  # stop at 50 predicted tokens\n",
    "    # Generate predictions\n",
    "    predictions, mems = model(tokens_tensor, mems=mems)\n",
    "\n",
    "    # Get most probable word index\n",
    "    predicted_index = torch.topk(predictions[0, -1, :], 1)[1]\n",
    "\n",
    "    # Extract the word from the index\n",
    "    predicted_token = tokenizer.decode(predicted_index)\n",
    "\n",
    "    # break if [EOS] reached\n",
    "    if predicted_token == tokenizer.eos_token:\n",
    "        break\n",
    "\n",
    "    # Store the current token\n",
    "    predicted_tokens.append(predicted_token)\n",
    "\n",
    "    # Append new token to the existing sequence\n",
    "    tokens_tensor = torch.cat((tokens_tensor, predicted_index.unsqueeze(1)), dim=1)\n",
    "\n",
    "print('Initial sequence: ' + text)\n",
    "print('Predicted output: ' + \" \".join(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryit",
   "language": "python",
   "name": "tryit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
