{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 长短期记忆（LSTM）\n",
    "\n",
    "\n",
    "本节将介绍另一种常用的门控循环神经网络：长短期记忆（long short-term memory，简称 LSTM）[1]。它比门控循环单元的结构稍微复杂一点。\n",
    "\n",
    "\n",
    "## 长短期记忆\n",
    "\n",
    "LSTM 中引入了三个门：输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞（某些文献把记忆细胞当成一种特殊的隐藏状态），从而记录额外的信息。\n",
    "\n",
    "\n",
    "### 输入门、遗忘门和输出门\n",
    "\n",
    "同门控循环单元中的重置门和更新门一样，如图 6.7 所示，长短期记忆的门的输入均为当前时间步输入 $\\boldsymbol{X}_t$ 与上一时间步隐藏状态 $\\boldsymbol{H}_{t-1}$，输出由激活函数为 sigmoid 函数的全连接层计算得到。如此一来，这三个门元素的值域均为 $[0,1]$。\n",
    "\n",
    "![长短期记忆中输入门、遗忘门和输出门的计算。](img/lstm_0.svg)\n",
    "\n",
    "具体来说，假设隐藏单元个数为 $h$，给定时间步 $t$ 的小批量输入 $\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数为 $n$，输入个数为 $d$）和上一时间步隐藏状态 $\\boldsymbol{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。\n",
    "时间步 $t$ 的输入门 $\\boldsymbol{I}_t \\in \\mathbb{R}^{n \\times h}$、遗忘门 $\\boldsymbol{F}_t \\in \\mathbb{R}^{n \\times h}$ 和输出门 $\\boldsymbol{O}_t \\in \\mathbb{R}^{n \\times h}$ 分别计算如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{I}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xi} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hi} + \\boldsymbol{b}_i),\\\\\n",
    "\\boldsymbol{F}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xf} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hf} + \\boldsymbol{b}_f),\\\\\n",
    "\\boldsymbol{O}_t &= \\sigma(\\boldsymbol{X}_t \\boldsymbol{W}_{xo} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{ho} + \\boldsymbol{b}_o),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中的 $\\boldsymbol{W}_{xi}, \\boldsymbol{W}_{xf}, \\boldsymbol{W}_{xo} \\in \\mathbb{R}^{d \\times h}$ 和 $\\boldsymbol{W}_{hi}, \\boldsymbol{W}_{hf}, \\boldsymbol{W}_{ho} \\in \\mathbb{R}^{h \\times h}$ 是权重参数，$\\boldsymbol{b}_i, \\boldsymbol{b}_f, \\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times h}$ 是偏差参数。\n",
    "\n",
    "\n",
    "### 候选记忆细胞\n",
    "\n",
    "接下来，长短期记忆需要计算候选记忆细胞 $\\tilde{\\boldsymbol{C}}_t$。它的计算同上面介绍的三个门类似，但使用了值域在 $[-1, 1]$ 的 tanh 函数做激活函数，如图 6.8 所示。\n",
    "\n",
    "![长短期记忆中候选记忆细胞的计算。](img/lstm_1.svg)\n",
    "\n",
    "\n",
    "具体来说，时间步 $t$ 的候选记忆细胞 $\\tilde{\\boldsymbol{C}}_t \\in \\mathbb{R}^{n \\times h}$ 的计算为\n",
    "\n",
    "$$\\tilde{\\boldsymbol{C}}_t = \\text{tanh}(\\boldsymbol{X}_t \\boldsymbol{W}_{xc} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hc} + \\boldsymbol{b}_c),$$\n",
    "\n",
    "其中的 $\\boldsymbol{W}_{xc} \\in \\mathbb{R}^{d \\times h}$ 和 $\\boldsymbol{W}_{hc} \\in \\mathbb{R}^{h \\times h}$ 是权重参数，$\\boldsymbol{b}_c \\in \\mathbb{R}^{1 \\times h}$ 是偏差参数。\n",
    "\n",
    "\n",
    "### 记忆细胞\n",
    "\n",
    "我们可以通过元素值域在 $[0, 1]$ 的输入门、遗忘门和输出门来控制隐藏状态中信息的流动：这一般也是通过使用按元素乘法（符号为 $\\odot$）来实现。当前时间步记忆细胞 $\\boldsymbol{C}_t \\in \\mathbb{R}^{n \\times h}$ 的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘门和输入门来控制信息的流动：\n",
    "\n",
    "$$\\boldsymbol{C}_t = \\boldsymbol{F}_t \\odot \\boldsymbol{C}_{t-1} + \\boldsymbol{I}_t \\odot \\tilde{\\boldsymbol{C}}_t.$$\n",
    "\n",
    "\n",
    "如图 6.9 所示，遗忘门控制上一时间步的记忆细胞 $\\boldsymbol{C}_{t-1}$ 中的信息是否传递到当前时间步，而输入门则可以控制当前时间步的输入 $\\boldsymbol{X}_t$ 通过候选记忆细胞 $\\tilde{\\boldsymbol{C}}_t$ 如何流入当前时间步的记忆细胞。如果遗忘门一直近似 1 且输入门一直近似 0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。\n",
    "\n",
    "![长短期记忆中记忆细胞的计算。这里的乘号是按元素乘法。](img/lstm_2.svg)\n",
    "\n",
    "\n",
    "### 隐藏状态\n",
    "\n",
    "有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态 $\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times h}$ 的信息的流动：\n",
    "\n",
    "$$\\boldsymbol{H}_t = \\boldsymbol{O}_t \\odot \\text{tanh}(\\boldsymbol{C}_t).$$\n",
    "\n",
    "这里的 tanh 函数确保隐藏状态元素值在 -1 到 1 之间。需要注意的是，当输出门近似 1 时，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似 0 时，记忆细胞信息只自己保留。图 6.10 展示了长短期记忆中隐藏状态的计算。\n",
    "\n",
    "![长短期记忆中隐藏状态的计算。这里的乘号是按元素乘法。](img/lstm_3.svg)\n",
    "\n",
    "\n",
    "## 读取数据集\n",
    "\n",
    "下面我们开始实现并展示长短期记忆。和前几节中的实验一样，我们依然使用周杰伦歌词数据集来训练模型作词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "\n",
    "本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来储存之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。\n",
    "\n",
    "\n",
    "## 不含隐藏状态的神经网络\n",
    "\n",
    "让我们考虑一个单隐藏层的多层感知机。给定样本数为 $n$、输入个数（特征数或特征向量维度）为 $d$ 的小批量数据样本 $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。设隐藏层的激活函数为 $\\phi$，那么隐藏层的输出 $\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$ 计算为\n",
    "\n",
    "$$\\boldsymbol{H} = \\phi(\\boldsymbol{X} \\boldsymbol{W}_{xh} + \\boldsymbol{b}_h),$$\n",
    "\n",
    "其中隐藏层权重参数 $\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏差参数 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，$h$ 为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量 $\\boldsymbol{H}$ 作为输出层的输入，且设输出个数为 $q$（例如分类问题中的类别数），输出层的输出为\n",
    "\n",
    "$$\\boldsymbol{O} = \\boldsymbol{H} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q,$$\n",
    "\n",
    "其中输出变量 $\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$, 输出层权重参数 $\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$, 输出层偏差参数 $\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。如果是分类问题，我们可以使用 $\\text{softmax}(\\boldsymbol{O})$ 来计算输出类别的概率分布。\n",
    "\n",
    "\n",
    "## 含隐藏状态的循环神经网络\n",
    "\n",
    "现在我们考虑输入数据存在时间相关性的情况。假设 $\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$ 是序列中时间步 $t$ 的小批量输入，$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$ 是该时间步的隐藏层变量。跟多层感知机不同的是，这里我们保存上一时间步的隐藏变量 $\\boldsymbol{H}_{t-1}$，并引入一个新的权重参数 $\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，当前时间步的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
    "\n",
    "$$\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).$$\n",
    "\n",
    "与多层感知机相比，我们在这里添加了 $\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$ 一项。由上式中相邻时间步的隐藏变量 $\\boldsymbol{H}_t$ 和 $\\boldsymbol{H}_{t-1}$ 之间的关系可知，这里的隐藏变量捕捉了截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了它在上一时间步相同的定义，上式的计算是循环的。使用循环计算的网络即循环神经网络。\n",
    "\n",
    "循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。如无特别说明，本章中的循环神经网络基于上式中隐藏状态的循环计算。在时间步 $t$，输出层的输出和多层感知机中的计算类似：\n",
    "\n",
    "$$\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.$$\n",
    "\n",
    "循环神经网络的参数包括隐藏层的权重 $\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ 和偏差 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，以及输出层的权重 $\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ 和偏差 $\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即便在不同时间步，循环神经网络始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的递增而增长。\n",
    "\n",
    "图 6.1 展示了循环神经网络在三个相邻时间步的计算逻辑。在时间步 $t$，隐藏状态的计算可以看成是将输入 $\\boldsymbol{X}_t$ 和前一时间步隐藏状态 $\\boldsymbol{H}_{t-1}$ 连结后输入一个激活函数为 $\\phi$ 的全连接层。该全连接层的输出就是当前时间步的隐藏状态 $\\boldsymbol{H}_t$，且模型参数为 $\\boldsymbol{W}_{xh}$ 与 $\\boldsymbol{W}_{hh}$ 的连结，偏差为 $\\boldsymbol{b}_h$。当前时间步 $t$ 的隐藏状态 $\\boldsymbol{H}_t$ 将参与下一个时间步 $t+1$ 的隐藏状态 $\\boldsymbol{H}_{t+1}$ 的计算，并输入到当前时间步的全连接输出层。\n",
    "\n",
    "![含隐藏状态的循环神经网络。](img/rnn.svg)\n",
    "\n",
    "我们刚刚提到，隐藏状态中 $\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$ 的计算等价于 $\\boldsymbol{X}_t$ 与 $\\boldsymbol{H}_{t-1}$ 连结后的矩阵乘以 $\\boldsymbol{W}_{xh}$ 与 $\\boldsymbol{W}_{hh}$ 连结后的矩阵。接下来，我们用一个具体的例子来验证这一点。首先，我们构造矩阵`X`、`W_xh`、`H`和`W_hh`，它们的形状分别为（3，1）、（1，4）、（3，2）和（2，4）。将`X`与`W_xh`、`H`与`W_hh`分别相乘，再把两个相乘的结果相加，得到形状为（3，4）的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd, autograd\n",
    "import random\n",
    "import zipfile\n",
    "from mxnet.gluon import loss as gloss\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"jaychou_lyrics.txt.zip\") as zin:\n",
    "    for file in zin.namelist():\n",
    "        with zin.open(file) as f:\n",
    "            corpus_chars = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n',' ').replace('\\r',' ')\n",
    "corpus_chars = corpus_chars[:10000]\n",
    "corpus_chars[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立字符索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_char = list(set(corpus_chars))\n",
    "vocab_size = len(index_to_char)\n",
    "char_to_index = {char:i for i,char in enumerate(index_to_char)}\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "index: [824, 97, 659, 481, 263, 293, 559, 824, 97, 107, 87, 364, 893, 291, 847, 206, 559, 824, 97, 107]\n"
     ]
    }
   ],
   "source": [
    "indices_corpus = [char_to_index[char] for char in corpus_chars]\n",
    "sample = indices_corpus[:20]\n",
    "print(\"chars: {}\".format(\"\".join(index_to_char[idx] for idx in sample)))\n",
    "print(\"index: {}\".format(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序数据的采样\n",
    "1. 随机采样\n",
    "2. 相邻采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机采样\n",
    "- 批量大小 batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数。\n",
    "- 在随机采样中，每个样本是原始序列上任意截 取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一 个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。\n",
    "- 在训练模型时，每次随机采样前都需要重新初始化隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(indices_corpus, batch_size, num_steps, ctx=None):\n",
    "    num_example = (len(indices_corpus) - 1) // num_steps    ## 总共有多少个样本\n",
    "    epoch_size = num_example // batch_size  ## 每个 epoch 有多少个小批量\n",
    "    example_indices = list(range(num_example)) ## 样本的索引\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    def _data(pos):\n",
    "        return indices_corpus[pos : pos + num_steps]\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        # 每次生成 batch_size 个随机样本\n",
    "        batch_start = i * batch_size\n",
    "        batch_indices = example_indices[batch_start:batch_start + batch_size]  ## 小批量的样本索引\n",
    "        X = [_data( j * num_steps ) for j in batch_indices]\n",
    "        Y = [_data( j * num_steps + 1) for j in batch_indices]\n",
    " \n",
    "        yield nd.array(X, ctx), nd.array(Y, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  \n",
      "[[12. 13. 14. 15. 16. 17.]\n",
      " [ 0.  1.  2.  3.  4.  5.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "Y: \n",
      "[[13. 14. 15. 16. 17. 18.]\n",
      " [ 1.  2.  3.  4.  5.  6.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "\n",
      "X:  \n",
      "[[18. 19. 20. 21. 22. 23.]\n",
      " [ 6.  7.  8.  9. 10. 11.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "Y: \n",
      "[[19. 20. 21. 22. 23. 24.]\n",
      " [ 7.  8.  9. 10. 11. 12.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相邻采样 \n",
    "除了对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。此时，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态， 从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响。一方面，在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态。另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图分离出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size  \n",
    "    \n",
    "    indices = corpus_indices[0: batch_size*batch_len].reshape((\n",
    "batch_size, batch_len))\n",
    "    \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  \n",
      "[[ 0.  1.  2.  3.  4.  5.]\n",
      " [15. 16. 17. 18. 19. 20.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "Y: \n",
      "[[ 1.  2.  3.  4.  5.  6.]\n",
      " [16. 17. 18. 19. 20. 21.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "\n",
      "X:  \n",
      "[[ 6.  7.  8.  9. 10. 11.]\n",
      " [21. 22. 23. 24. 25. 26.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "Y: \n",
      "[[ 7.  8.  9. 10. 11. 12.]\n",
      " [22. 23. 24. 25. 26. 27.]]\n",
      "<NDArray 2x6 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6): \n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络的从零开始实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_onehot(X,size):\n",
    "    return [nd.one_hot(x,size) for x in X.T]\n",
    "X = nd.arange(10).reshape((2, 5))\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "len(inputs), inputs[0].shape  # time_step = 5，batch_size = 2, features = 2582"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "- num_hiddens 是hidden cell的差参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = None\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)),\n",
    "                        _one((num_hiddens, num_hiddens)),\n",
    "                        nd.zeros(num_hiddens, ctx=ctx))\n",
    "    # 隐藏层参数\n",
    "    W_xi, W_hi, b_i = _three() #输入门\n",
    "    W_xf, W_hf, b_f = _three() #遗忘门\n",
    "    W_xo, W_ho, b_o = _three() #输出门\n",
    "    W_xc, W_hc, b_c = _three() #候选cell参数 \n",
    "    # 输出层参数\n",
    "    \n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    \n",
    "    # 附上梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "b_c, W_hq, b_q]\n",
    "    \n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx))\n",
    "\n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "             W_hq, b_q] = params\n",
    "    H,C = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = nd.sigmoid(nd.dot(X, W_xi) + nd.dot(H, W_hi) + b_i)\n",
    "        F = nd.sigmoid(nd.dot(X, W_xf) + nd.dot(H, W_hf) + b_f)\n",
    "        O = nd.sigmoid(nd.dot(X, W_xo) + nd.dot(H, W_ho) + b_o)\n",
    "        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * C.tanh()\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_lstm_state(X.shape[0], num_hiddens, ctx = None)\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = lstm(inputs, state, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027), (2, 256))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义预测函数\n",
    "以下函数基于前缀 prefix(含有数个字符的字符串)来预测接下来的 num_chars个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(prefix, num_chars, rnn, params, init_lstm_state,\n",
    "                        num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    state = init_lstm_state(1, num_hiddens, ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    \n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入。\n",
    "        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n",
    "        (Y, state) = lstm(X, state, params)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    \n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开蟑榜沉篇来他已密明漠'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_lstm('分开', 10, lstm, params, init_lstm_state, num_hiddens, vocab_size, ctx, index_to_char, char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度剪裁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx = None):\n",
    "    norm = nd.array([0.0], ctx) \n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum() \n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params: \n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size): # 本函数已保存在 gluonbook 包中方便以后使用。 \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,\n",
    "                                   vocab_size, corpus_indices, idx_to_char,\n",
    "                                   char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                                   lr, clipping_theta, batch_size, pred_period,\n",
    "                                   pred_len, prefixes, ctx = None):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = data_iter_consecutive\n",
    "    \n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter: # 如使用相邻采样，在 epoch 开始时初始化隐藏状态。\n",
    "            state = init_lstm_state(batch_size, num_hiddens, ctx)\n",
    "        \n",
    "        loss_sum, start = 0.0, time.time()\n",
    "        data_iter = data_iter_fn(indices_corpus, batch_size, num_steps, ctx)\n",
    "        for t, (X, Y) in enumerate(data_iter):\n",
    "            if is_random_iter: # 如使用随机采样，在每个小批量更新前初始化隐藏状态。 \n",
    "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "            else:\n",
    "                # 否则需要使用 detach 函数从计算图分离隐藏状态\n",
    "                for s in state: \n",
    "                    s.detach()\n",
    "        \n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # outputs 有 num_steps 个形状为(batch_size，vocab_size)的矩阵。 \n",
    "                (outputs, state) = lstm(inputs, state, params)\n",
    "                # 拼接之后形状为(num_steps * batch_size，vocab_size)。\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # Y 的形状是(batch_size，num_steps)，转置后再变成⻓度为\n",
    "                # batch * num_steps 的向量，这样跟输出的行一一对应。\n",
    "                y = Y.T.reshape((-1,))\n",
    "                # 使用交叉熵损失计算平均分类误差。\n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, ctx) # 裁剪梯度。 gb.sgd(params, lr, 1) # 因为误差已经取过均值，梯度不用再做平均。 loss_sum += l.asscalar()\n",
    "            sgd(params, lr, 1) # 因为误差已经取过均值，梯度不用再做平均。 \n",
    "            loss_sum += l.asscalar()\n",
    "            \n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, math.exp(loss_sum / (t + 1)), time.time() - start)) \n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_lstm(prefix, pred_len, lstm, params, init_lstm_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f64f9ba1a740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpred_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'分开'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'不分开'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                    \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_to_char\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                    \u001b[0mchar_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 200, 35, 32, 1e2, 1e-2 \n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "\n",
    "train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,\n",
    "                                   vocab_size, indices_corpus, index_to_char,\n",
    "                                   char_to_index, False, num_epochs, num_steps,\n",
    "                                   lr, clipping_theta, batch_size, pred_period,\n",
    "                                   pred_len, prefixes, ctx = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
